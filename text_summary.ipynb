{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''8月4日晚，市委书记、市疫情防控指挥部第一指挥长王立主持召开市委常委会会议暨市疫情防控指挥部会议，传达学习习近平总书记关于当前疫情防控工作的重要指示精神、李克强总理批示要求及省委省政府工作部署，总结前一段工作情况，对下一阶段疫情防控工作进行再研究、再部署、再落实。王立指出，近期国内部分地区疫情传播链不断延长，且发展趋势仍具不确定性，必须引起高度重视、高度警觉。全市上下要认真学习领会、坚决贯彻落实习近平总书记重要指示精神，把疫情防控作为当前工作重中之重，深刻认识抓好疫情防控是践行人民至上、生命至上的根本要求，是保持经济恢复良好势头的重要前提，思想上高度重视、克服松懈麻痹思想，工作中紧急动员、紧急行动起来，落实时坚决有力、毫不含糊，认真落实“四早”要求和“四方责任”，深刻反思当前我市疫情防控工作还存在的漏洞短板，举一反三，全面整改，以全力以赴的应对保障人民群众健康安全，以实际行动做到“两个维护”。王立强调，要用好抗疫大战大考中行之有效的经验做法，严格落实“快、狠、严、扩、足”工作要求，确保一旦出现苗头性问题，能够在最短时间内将疫情控制、解决在最小范围。要加强疫情防控信息互联互通，依托信息化、大数据等手段，抓好境外、中高风险地区来宜返宜人员以及其他密接者、次密接者的精准摸排，查清传播链条，形成责任闭环、工作闭环、管控闭环。要全面、快速推进新冠疫苗的接种，做好采样机构、人员、物资、场所等各项准备工作，确保具备在48小时内完成全市人员核酸检测能力。要确保疫情防控指挥机制高效运转，未发生疫情时关键抓“防”，一旦发生疫情关键要“快”，迅速切断传播途径和链条。要严格执行24小时值班制度和领导带班制度，确保应急响应和处置措施跑在病毒蔓延扩散之前。王立指出，要紧盯重点地区和环节。坚持人物地同防，对火车站、汽车站、高速路出入口、机场、港口等实行闭环管理和全面消杀，对重点岗位人员进行常态化核酸检测、健康监测。定点医院、隔离场所等机构要规范“三区两通道”设置，全面加强预检分诊和发热门诊管理，抓好集中隔离点建设，规范做好废弃物处置。加强冷链物流监管，落实好追踪溯源机制，确保冷链等流通环节安全。守好小区重要防线，让群众紧张起来，紧急行动起来。人员聚集场所、公共场所等要严格落实戴口罩、一米线、测体温、亮码通行等防控措施。养老院、监所等特殊场所要严格管控，确保“零感染”。要用好新媒体、“小喇叭”等加强宣传教育，强化舆情监测，积极回应社会关切。对于恶意造谣传谣、扰乱防疫秩序的，要及时依法依规查处。\n",
    "　　王立要求，要从严落实属地责任、主体责任、包保责任、监管责任，守土尽责巩固拓展决定性成果。各级各部门要树牢阵地意识，党委书记要履行第一责任人责任，切实做到守土担责不含糊、守土尽责不懈怠，出手要快、考虑要全、督查要细、工作要实、要求要严，以同时间赛跑的状态扎实做好各项工作，宁可往前一步形成交叉、不可后退半步形成空当，确保疫情防控各项措施落地落实。企业和单位是做好本单位疫情防控的责任主体，要将防控措施科学化、精细化、实用化，绝不能作壁上观、袖手旁观。打好打赢疫情防控的人民战争、总体战、阻击战，主体是人民群众，关键靠党员干部。全市广大党员干部要切实当好战斗者、守护者，以高度的思想自觉行动自觉把各项防控措施抓实抓细抓到位，坚决杜绝形式主义、官僚主义。对认识不到位、举措不具体、作风不扎实的行为，要一律从严从重查处。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "sentences_local = []\n",
    "def split_sentences(text,p='[。.？：]',filter_p='\\s+'):\n",
    "    f_p = re.compile(filter_p)\n",
    "    text = re.sub(f_p,'',text)\n",
    "    pattern = re.compile(p)\n",
    "    split = re.split(pattern,text)\n",
    "    return len(split)\n",
    "for i in paragraph:\n",
    "    split_len = split_sentences(i)\n",
    "    sentences_local.append(split_len)\n",
    "print(sentences_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_sentences(text,p='[。! ？]',filter_p='\\s+'):\n",
    "    f_p = re.compile(filter_p)\n",
    "    text = re.sub(f_p,'',text)\n",
    "    pattern = re.compile(p)\n",
    "    split = re.split(pattern,text)\n",
    "    return split\n",
    "\n",
    "print(split_sentences(text)[:10])\n",
    "split_list = split_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_sentences(text,p='[。.？：]',filter_p='\\s+'):\n",
    "    f_p = re.compile(filter_p)\n",
    "    text = re.sub(f_p,'',text)\n",
    "    pattern = re.compile(p)\n",
    "    split = re.split(pattern,text)\n",
    "    return split\n",
    "\n",
    "print(split_sentences(text)[:10])\n",
    "split_list = split_sentences(text)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(split_list, columns=['content']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_local_total = []\n",
    "for lable in sentences_local:\n",
    "    list_temp = [i for i in range(lable)]\n",
    "    sentences_local_total.extend(list_temp)\n",
    "sentences_local_total\n",
    "lag = 0\n",
    "paragraph_local = []\n",
    "for i in sentences_local:\n",
    "    paragraph_local.extend([lag]*i)\n",
    "    # print(paragraph_local)\n",
    "    lag = lag + 1\n",
    "df['sentences_local'] = pd.DataFrame(sentences_local_total, columns=['sentences_local']) \n",
    "df['paragraph_local'] = pd.DataFrame(paragraph_local, columns=['paragraph_local']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "#定义分词函数\n",
    "def cut(text): return ' '.join(jieba.cut(text)) \n",
    "\n",
    "df['tokenized_content'] = df['content'].apply(cut)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer() #初始化\n",
    "dataset = []\n",
    "tfidf_sum_score = []\n",
    "for sen in split_list:\n",
    "    dataset.append(' '.join(jieba.lcut(sen))) \n",
    "tfidf.fit(dataset) \n",
    "#print(tfidf.get_feature_names()) ##词袋\n",
    "#print(tfidf.vocabulary_) ## 词汇表\n",
    "corpus_vector = tfidf.transform(dataset).toarray()\n",
    "print(len(corpus_vector))\n",
    "for i in range(0,len(corpus_vector)):\n",
    "    tfidf_sum_score.append(sum(corpus_vector[i]))\n",
    "df['tfidf_sum_score'] = pd.DataFrame(tfidf_sum_score, columns=['tfidf_sum_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "def num(text):    \n",
    "    words =pseg.cut(text)\n",
    "    outStr= []\n",
    "    n_num = v_num =0\n",
    "    for x in words:\n",
    "        outStr.append([x.word,x.flag])\n",
    "    for i in range(0,len(outStr)):\n",
    "        if (outStr[i][1]=='n'):\n",
    "            n_num = n_num + 1\n",
    "        if (outStr[i][1]=='v'):\n",
    "            v_num = v_num + 1\n",
    "    return  n_num+v_num\n",
    "df['num'] = df['content'].apply(num)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "def length(text):    \n",
    "    return  len(text)\n",
    "df['length'] = df['content'].apply(length)\n",
    "df = df[~df['length']<=6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg'] = df['num']/df['length']\n",
    "df=df.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "def translator(text):\n",
    "    return  bc.encode([text])\n",
    "df['vetor'] = df['content'].apply(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def sim(text):\n",
    "    sim_list = []\n",
    "    for i in range(len(df['vetor'])):\n",
    "        sim_list.extend(cosine_similarity(text,df['vetor'][i]))\n",
    "    return  sim_list\n",
    "df['sim'] = df['vetor'].apply(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vetor_list = df['vetor'].tolist()\n",
    "vetor_list_all = []\n",
    "for i in vetor_list:\n",
    "    for j in i:\n",
    "        vetor_list_all.extend(j)\n",
    "from numpy import *\n",
    "vetor_list_all_array = array(vetor_list_all)\n",
    "# vetor_list_all_array = vetor_list_all.reshape(768,)\n",
    "vetor_list_all_array = vetor_list_all_array.reshape(len(df),768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['vetor'][1].tolist()[0])\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "vector_list_pd = pd.DataFrame(vetor_list_all_array)\n",
    "#假如我要构造一个聚类数为12的聚类器\n",
    "for i in range(5,6):\n",
    "    print('聚类数：',i)\n",
    "    estimator = KMeans(n_clusters=i, random_state=777)#构造聚类器,设定随机种子\n",
    "    estimator.fit(vector_list_pd)#聚类\n",
    "\n",
    "    r1 = pd.Series(estimator.labels_).value_counts()  #统计各个类别的数目\n",
    "    print(r1)\n",
    "    r2 = pd.DataFrame(estimator.cluster_centers_)     #找出聚类中心\n",
    "    r = pd.concat([r2, r1], axis = 1) #横向连接（0是纵向），得到聚类中心对应的类别下的数目\n",
    "    r.columns = list(vector_list_pd.columns) + [u'类别数目'] #重命名表头\n",
    "    # print(r)\n",
    "    print(\"轮廓系数：\", metrics.silhouette_score(vector_list_pd, estimator.labels_, metric='euclidean'))\n",
    "    # print(vector_list_pd, estimator.labels_)\n",
    "    df['k-means_cluster'] = estimator.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_num'] = df['tfidf_sum_score']/df['length']\n",
    "df[df['k-means_cluster']==0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(df['k-means_cluster'].unique()):\n",
    "    if df['k-means_cluster'].value_counts()[i] <(len(df)/10):\n",
    "        df['k-means_cluster'].isin([i])\n",
    "        df2 = df[~df['k-means_cluster'].isin([i])]\n",
    "        df2.reset_index(drop=True, inplace=True)\n",
    "def process(vetor):\n",
    "    return  vetor.tolist()[0]\n",
    "df['new_vetor'] = df['vetor'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(text):\n",
    "    sim_list = []\n",
    "    for i in range(len(df['vetor'])):\n",
    "        sim_list.extend(cosine_similarity(text,df['vetor'][i])[0])\n",
    "    return  sim_list\n",
    "df['sim'] = df['vetor'].apply(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= []\n",
    "for i in df['vetor'][1].tolist()[0]:\n",
    "    data2.append(i-sum(df['vetor'][1].tolist()[0])/768)\n",
    "data3= []\n",
    "for i in title_vetor.tolist()[0]:\n",
    "    data3.append(i-sum(title_vetor.tolist()[0])/768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "d2 = []\n",
    "for i in range(len(df['vetor'])):\n",
    "    data2= []\n",
    "    for j in df['vetor'][i].tolist()[0]:\n",
    "        data2.append(j-sum(df['vetor'][1].tolist()[0])/768)\n",
    "    data3= []\n",
    "    for j in title_vetor.tolist()[0]:\n",
    "        data3.append(j-sum(title_vetor.tolist()[0])/768)\n",
    "    sim_temp = 1-pdist([data2,data3],'cosine')\n",
    "    sim_temp = sim_temp.tolist()\n",
    "    d2.extend(sim_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['title_sim'] = d2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log)\n",
    "x = np.array([df['sentences_local']+0.15])\n",
    "weight = transformer.transform(x)\n",
    "def MaxMinNormalization(x,Max,Min):\n",
    "\tx = (x - Min)/ (Min - Max);\n",
    "\treturn x;\n",
    "sentences_local_Nor =  (MaxMinNormalization(weight,weight.min(),weight.max()))*-1\n",
    "df['sentences_local_Nor'] = sentences_local_Nor.tolist()[0]\n",
    "\n",
    "weight = np.array(df['avg'])\n",
    "sentences_local_Nor =  (MaxMinNormalization(weight,weight.max(),weight.min()))*-1\n",
    "df['num_avg_Nor'] = sentences_local_Nor.tolist()\n",
    "\n",
    "weight = np.array(df['avg_num'])\n",
    "sentences_local_Nor =  (MaxMinNormalization(weight,weight.max(),weight.min()))*-1\n",
    "df['tfidf_avg_Nor'] = sentences_local_Nor.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim_list = []\n",
    "for i in range(len(df)):\n",
    "    sim_list.extend([np.array(df['sim'][i]).sum()-1])\n",
    "weight = np.array(sim_list)-np.array(sim_list).min()\n",
    "sentences_local_Nor =  (MaxMinNormalization(weight,weight.max(),weight.min())*-1)\n",
    "df['sim_Nor'] = sentences_local_Nor.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df2=df[df['length']>=6]\n",
    "df3 = df2[['content','length', 'k-means_cluster','avg_num','title_sim','sentences_local_Nor', 'tfidf_avg_Nor','num_avg_Nor', 'sim_Nor']]\n",
    "df3['all_score_add'] = df3['avg_num']+df3['tfidf_avg_Nor']+df3['num_avg_Nor']+df3['sim_Nor']+df['sentences_local_Nor']\n",
    "df3['all_score_mult'] = df3['avg_num']*df3['tfidf_avg_Nor']*df3['num_avg_Nor']+df3['sim_Nor']*df['sentences_local_Nor']\n",
    "df3.sort_values(\"all_score_add\",inplace=True,ascending=False)\n",
    "df3[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.groupby('k-means_cluster').head(3)\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data=['a','b','c','a','a','b']\n",
    "data1=np.array(data)\n",
    "#计算信息熵的方法\n",
    "def calc_ent(x):\n",
    "    \"\"\"\n",
    "        calculate shanno ent of x\n",
    "    \"\"\"\n",
    "\n",
    "    x_value_list = set([x[i] for i in range(x.shape[0])])\n",
    "    ent = 0.0\n",
    "    for x_value in x_value_list:\n",
    "        p = float(x[x == x_value].shape[0]) / x.shape[0]\n",
    "        logp = np.log2(p)\n",
    "        ent -= p * logp\n",
    "    print (ent,x_value_list)\n",
    "    return ent,x_value_list\n",
    "calc_ent(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_content = ''.join(list(df4['content']))\n",
    "summary_content\n",
    "words =pseg.cut(text)\n",
    "outStr= []\n",
    "n_num = v_num =0\n",
    "for x in words:\n",
    "    outStr.append([x.word,x.flag])\n",
    "summary_text_cut = []\n",
    "summary_text_len = 0\n",
    "for i in df4['content']:\n",
    "    summary_text_cut.extend(jieba.cut(i))\n",
    "    summary_text_len = summary_text_len+len(i)\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "summary_vetor = bc.encode([summary_content])\n",
    "cosine_similarity(summary_vetor,title_vetor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data=['a','b','c','a','a','b']\n",
    "data = summary_text_cut\n",
    "data1=np.array(data)\n",
    "#计算信息熵的方法\n",
    "def calc_ent(x):\n",
    "    \"\"\"\n",
    "        calculate shanno ent of x\n",
    "    \"\"\"\n",
    "\n",
    "    x_value_list = set([x[i] for i in range(x.shape[0])])\n",
    "    ent = 0.0\n",
    "    for x_value in x_value_list:\n",
    "        p = float(x[x == x_value].shape[0]) / x.shape[0]\n",
    "        logp = np.log2(p)\n",
    "        ent -= p * logp\n",
    "    return [ent,x_value_list,ent/summary_text_len]\n",
    "calc_ent(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********\n(0.11259996027579887, '王立指出，要紧盯重点地区和环节')\n(0.09087958787617295, '守好小区重要防线，让群众紧张起来，紧急行动起来')\n(0.08998026983616063, '养老院、监所等特殊场所要严格管控，确保“零感染”')\n(0.07517038370074192, '对于恶意造谣传谣、扰乱防疫秩序的，要及时依法依规查处')\n(0.06513658566190891, '对认识不到位、举措不具体、作风不扎实的行为，要一律从严从重查处')\n(0.05646779254518945, '要加强疫情防控信息互联互通，依托信息化、大数据等手段，抓好境外、中高风险地区来宜返宜人员以及其他密接者、次密接者的精准摸排，查清传播链条，形成责任闭环、工作闭环、管控闭环')\n(0.05530157383742847, '王立强调，要用好抗疫大战大考中行之有效的经验做法，严格落实“快、狠、严、扩、足”工作要求，确保一旦出现苗头性问题，能够在最短时间内将疫情控制、解决在最小范围')\n(0.052153010723663226, '加强冷链物流监管，落实好追踪溯源机制，确保冷链等流通环节安全')\n*********\n(0.08106621514126906, '全市上下要认真学习领会、坚决贯彻落实习近平总书记重要指示精神，把疫情防控作为当前工作重中之重，深刻认识抓好疫情防控是践行人民至上、生命至上的根本要求，是保持经济恢复良好势头的重要前提，思想上高度重视、克服松懈麻痹思想，工作中紧急动员、紧急行动起来，落实时坚决有力、毫不含糊，认真落实“四早”要求和“四方责任”，深刻反思当前我市疫情防控工作还存在的漏洞短板，举一反三，全面整改，以全力以赴的应对保障人民群众健康安全，以实际行动做到“两个维护”')\n(0.0697282314109297, '各级各部门要树牢阵地意识，党委书记要履行第一责任人责任，切实做到守土担责不含糊、守土尽责不懈怠，出手要快、考虑要全、督查要细、工作要实、要求要严，以同时间赛跑的状态扎实做好各项工作，宁可往前一步形成交叉、不可后退半步形成空当，确保疫情防控各项措施落地落实')\n(0.06355596365794398, '8月4日晚，市委书记、市疫情防控指挥部第一指挥长王立主持召开市委常委会会议暨市疫情防控指挥部会议，传达学习习近平总书记关于当前疫情防控工作的重要指示精神、李克强总理批示要求及省委省政府工作部署，总结前一段工作情况，对下一阶段疫情防控工作进行再研究、再部署、再落实')\n(0.05454791136356209, '王立强调，要用好抗疫大战大考中行之有效的经验做法，严格落实“快、狠、严、扩、足”工作要求，确保一旦出现苗头性问题，能够在最短时间内将疫情控制、解决在最小范围')\n(0.052827412593025744, '要加强疫情防控信息互联互通，依托信息化、大数据等手段，抓好境外、中高风险地区来宜返宜人员以及其他密接者、次密接者的精准摸排，查清传播链条，形成责任闭环、工作闭环、管控闭环')\n(0.04933502383682062, '要全面、快速推进新冠疫苗的接种，做好采样机构、人员、物资、场所等各项准备工作，确保具备在48小时内完成全市人员核酸检测能力')\n(0.049044695516471885, '坚持人物地同防，对火车站、汽车站、高速路出入口、机场、港口等实行闭环管理和全面消杀，对重点岗位人员进行常态化核酸检测、健康监测')\n(0.048656385949996876, '定点医院、隔离场所等机构要规范“三区两通道”设置，全面加强预检分诊和发热门诊管理，抓好集中隔离点建设，规范做好废弃物处置')\n*********\n(0.07292199850507487, '王立强调，要用好抗疫大战大考中行之有效的经验做法，严格落实“快、狠、严、扩、足”工作要求，确保一旦出现苗头性问题，能够在最短时间内将疫情控制、解决在最小范围')\n(0.06301271996212547, '全市上下要认真学习领会、坚决贯彻落实习近平总书记重要指示精神，把疫情防控作为当前工作重中之重，深刻认识抓好疫情防控是践行人民至上、生命至上的根本要求，是保持经济恢复良好势头的重要前提，思想上高度重视、克服松懈麻痹思想，工作中紧急动员、紧急行动起来，落实时坚决有力、毫不含糊，认真落实“四早”要求和“四方责任”，深刻反思当前我市疫情防控工作还存在的漏洞短板，举一反三，全面整改，以全力以赴的应对保障人民群众健康安全，以实际行动做到“两个维护”')\n(0.05811287186701518, '要全面、快速推进新冠疫苗的接种，做好采样机构、人员、物资、场所等各项准备工作，确保具备在48小时内完成全市人员核酸检测能力')\n(0.05781608903244475, '要加强疫情防控信息互联互通，依托信息化、大数据等手段，抓好境外、中高风险地区来宜返宜人员以及其他密接者、次密接者的精准摸排，查清传播链条，形成责任闭环、工作闭环、管控闭环')\n(0.05604741610805366, '定点医院、隔离场所等机构要规范“三区两通道”设置，全面加强预检分诊和发热门诊管理，抓好集中隔离点建设，规范做好废弃物处置')\n(0.04996120500018913, '养老院、监所等特殊场所要严格管控，确保“零感染”')\n(0.046887692428608026, '企业和单位是做好本单位疫情防控的责任主体，要将防控措施科学化、精细化、实用化，绝不能作壁上观、袖手旁观')\n(0.046547886771159265, '要用好新媒体、“小喇叭”等加强宣传教育，强化舆情监测，积极回应社会关切')\n"
     ]
    }
   ],
   "source": [
    "from nlg_yongzhuo import *\n",
    "import  jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "doc = text\n",
    "# fs可以填其中一个或几个 text_pronouns, text_teaser, mmr, text_rank, lead3, lda, lsi, nmf\n",
    "# for i in [text_pronouns, text_teaser, mmr, text_rank, lead3, lda, lsi, nmf]:\n",
    "for i in [text_teaser,mmr,text_rank]:\n",
    "    print('*'*9)\n",
    "    res_score = text_summarize(doc, fs=[i])\n",
    "    summary_text_cut_other = []\n",
    "    summary_text_other = [] \n",
    "    summary_text_other_len = 0\n",
    "    for rs in res_score[:8]:\n",
    "        print(rs)\n",
    "        summary_text_other_len = summary_text_other_len + len(rs[1])\n",
    "        summary_text_cut_other.extend(jieba.cut(rs[1]))\n",
    "        summary_text_other.extend(rs[1])\n",
    "    data2=np.array(summary_text_other)\n",
    "    # shang,shang_list,_ = calc_ent(data2)\n",
    "    summary_text_other = ''.join(summary_text_other)\n",
    "    # summary_text_other_vetor = bc.encode([summary_text_other])\n",
    "    # sim_other = cosine_similarity(summary_text_other_vetor,title_vetor)\n",
    "    # print(summary_text_other,shang,'*'*3,shang/float(summary_text_other_len),'*'*3,sim_other,'\\n','*'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}